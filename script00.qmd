---
title: "Fortune 1000"
authors:
  - name: Christophe Benavent
    department: DRM
    email: Christophe.benavent@dauphine.psl.eu
    affiliation: Université Dauphine - PSL
abstract: | 
  Ce papier introduit à l'utilisation de r dans l'environnement quarto pour les analyses statistiques fondamentales de la recherche en gestion. Il sert aussi de modèle (template) pour les travaux des étudiants.
  On examine des distributions, on calcule des moyennes, on les compare et on les teste, on calcule des corrélations. On explore quelques techniques d'analyses de données à la française et on conclue sur un peu d'économétrie
execute:
  code-fold: true
  message: false
  warning: false
keywords: 
  - r stat
  - ggplot
  - tests
  - anova
  - regression
  - clustering
  - fortune
bibliography: biblio.bib
biblio-style: unsrt
output: rticles::arxiv_article
---

{{< pagebreak >}}

# Introduction

Le cas que nous allons traité est celui des classements Fortune 1000 pour 2024 et 2023. On va se donner un but d'étude : examiner quels facteurs affectent la capitalisation boursière de ces 1000 entreprises. 


Il nous faut des outils, les packages, et des données que l'on va mettre en forme. 

## Les packages

La boite à outils essentielle est `tidyverse`, de @wickham_welcome_2019. On en découvrira l'[univers ici](https://www.tidyverse.org/packages/), elle nous permettra de manipuler les tableaux de données, et de produire des graphes élégant. Elle s'appuie sur une grammaire des données et une grammaire des graphiques. 

```{r}
library(tidyverse) #outil de base(dplyr+ggplot+...)
library(rstatix) # pour tout les test en tidy...
library(FactoMineR) #ACP AFC et clustering
library(factoextra)
library(flextable) #pour faire de jolis tableaux
library(scales) #un accessoire de ggplot
library(jtools) # pour de meilleures présentations des résultats des modèles de régressions
library(ggrepel) # un complément de ggplot utile 

```

## Lecture du fichier

Voici la [source des données](https://www.kaggle.com/datasets/jeannicolasduval/2024-fortune-1000-companies) du Fortune 1000 de 2024 et celles de [2023](https://www.kaggle.com/datasets/jeannicolasduval/2023-fortune-1000-companies-info) . On charge les fichiers et on les compile en un seul `dataframe`, le format ordinaire des données que l'on manipule en `r`. On en profite pour introduire quelques manipulations de données avec [`dplyr`](https://dplyr.tidyverse.org/), un des packages clé de la suite `tidyverse`. 

```{r}

df1 <- read_csv("fortune1000_2024.csv")|>
  mutate(Year="2024") |>
  select(-Worlds_Most_Admired_Companies)%>%
  rename(MarketCap_March=MarketCap_March28_M,
         Best_Companies=Best_Companies_to_Work_For)

df2 <- read_csv("fortune1000_2023.csv")|>
  mutate(Year="2023")%>%
  rename(MarketCap_March=MarketCap_March31_M)

df_tot<- rbind(df1,df2) #le fichier superpose l'ensemble des observations

```

Une autre stratégie pourrait être de s'intéresser aux entreprises du classement 2024, mais en les enrichissant des données de 2023 ( si elle étaient déjà dans le classements. On réalise une jointure à gauche (les données de 2024 sont mises à jour avec les données 2023). On utilise `dplyr`pour effectuer l'opération. Pour les jointures on consultera [ce document](https://perso.ens-lyon.fr/lise.vaudor/combinaisons-et-jointures-de-tables-avec-dplyr/).

```{r}
#ici on met à jour les données de 2024 avec celles de 2023
df_2024 <- df1 %>%
  left_join(df2, 
            by = "Company",
            suffix = c(".23", ".24"))

foo<-df_2024%>%
  select(Rank.24,Company, Rank.23,MarketCap_Updated_M.24, Number_of_employees.24, Revenues_M.24)%>%
  filter(Rank.24<21)%>%
  arrange(Rank.24)%>%
  mutate(cap_per_employee=MarketCap_Updated_M.24/Number_of_employees.24)

#ici on utilise flextable pour faire de jolis tableaux
ft <- flextable(foo)
ft <- theme_vanilla(ft)
ft <- add_footer_lines(ft, "")
ft <- color(ft, part = "footer", color = "#666666")
ft <- set_caption(ft, caption = "Les 20 premiers du Fortune1000 2024")
ft
```

# Analyse univariée


## Une variable quantitative

On étudie la capitalisation des entreprises en 2024. Examinons les statistiques essentielles : les centralités et les dispersions.

```{r}
mean<-round(mean(df1$MarketCap_March, na.rm=TRUE)/1000,1)
median<-round(median(df1$MarketCap_March, na.rm=TRUE)/1000,1)

```


On obtient donc : 

-  capitalisation moyenne : `{r} mean` milliards de $
-  capitalisation médiane: `{r} median` milliards de $
-  écart type : 

La distribution peut être représentée par un simple histogramme.

```{r}
ggplot(df1, aes(x=MarketCap_March ))+
  geom_histogram()

```
 
Avec quelques améliorations on obtient ceci:

```{r}
ggplot(df1, aes(x=MarketCap_March/1000 ))+ #les données sont en millions , on les transforme en milliards de$
  geom_histogram(fill="pink2", alpha=.5)+
  labs(title="Distribution du chiffre d'affaires des 1000 de Fortune (2024)",
       y = "Fréquence",
       x="Chiffre d'affaires (en milliards de $)")+
  theme_minimal()+
  scale_x_continuous(trans='log10', labels = scales::label_comma())
```

## Analyser la distribution d'une variable qualitative

On commence à compter le nombre d'occurrences par modalité. On analyse la variable secteur, on veut connaitre la fréquences des 1000 parmi les secteurs d'activités. 

```{r}

sector<- table(df1$Sector) |> #c'est une fonction de base de r
  as.data.frame() |> # le résultat de table est un vecteur, un data frame est un format plus utile
  arrange(desc(Freq))

ft<-flextable(sector)

ft
```

Un diagramme en barre

```{r}

ggplot(df1, aes(x=Sector, group= Year))+
  geom_bar(alpha=.5, fill="pink2")+
             coord_flip()+
  theme_minimal()

```
en mieux

```{r}
foo<-df1 %>%
  group_by(Sector)|>
  summarize(n=n())

ggplot(foo, aes(x=reorder(Sector,n), y=n))+
  geom_bar(stat="identity",alpha=.5, fill="pink2")+
             coord_flip()+
  theme_minimal()

```


# Comparons les moyennes

On va comparer les capitalisations médianes par secteur et par année. 

```{r}
foo<- df_tot |>
  group_by(Sector, Year)%>%
  summarise(n=n(),
            median=median(MarketCap_Updated_M, na.rm=TRUE) )

ggplot(foo, aes(x=reorder(Sector,median), y=median, group=Year))+geom_line(aes(color=Year))+ coord_flip()
```
Un test d'analyse de variance serait bienvenu. Ses résultats sont clairs : si on peut rejeter avec certitude l'hypothèse de moyennes égales entre tous les secteurs, et donc accepter que certains ont des moyenne plus fortes ou plus plaibles que les autres, et qu'il y a donc un effet sectoriel, la différence d'une année à l'autre n'est pas significative. Une p-value de .1724 indique un risque élevé que la réalité soit une absence de différence, c'est une hypothèse qu'on ne peut écarter.En moyenne les capitalisations n'ont pas changé.

```{r}
fit<-lm(MarketCap_Updated_M~Sector+Year, df_tot)
anova(fit)
```

# Corrélation de deux variables quantitatives

La capitalisation est l'actualisation du profit. On s'attend à ce que les deux variables soient fortement corrélées. 

```{r}
foo <- df_tot |>
  filter(!is.na(Profits_M) & !is.na(MarketCap_March))

r <-round(cor(foo$Profits_M,foo$MarketCap_March),3)

ggplot(foo, aes(x=Profits_M, y= MarketCap_March))+
  geom_point(alpha=.5, size=1)+
  geom_text_repel(aes(label=Company), size=3)+
  geom_smooth(method="lm", color="pink")+
  geom_smooth(method="loess", color="purple")+
  labs(title=paste0("corrélation : ", r))

```

# Comparaison plusieurs groupes sur une variable quanti

Comparer 2023 et 2024.

```{r}
ggplot(df_tot, aes(x=MarketCap_March, group=Year ))+
  geom_density(aes(color=Year))+
  scale_x_log10()

```

test en t (student)

```{r}
stat.test <- df_tot %>% 
  t_test(MarketCap_March ~ Year) %>%
  add_significance()
stat.test

```

# Analyser deux variables qualitative

## Tableau croisé

Le point de départ est l'analyse du tableau qui croise les deux variables

```{r}
library(flextable)
Crosstabs<-table(df1$HeadquartersState, df1$Sector)
#flextable(Crosstabs)
# Calculate the chi-square statistic
chi_square_test <- chisq.test(Crosstabs)

# Print the results
print(chi_square_test)
```

## Analyse des correspondances

```{r}
CT<-Crosstabs
ca=CA(CT, graph=FALSE)

fviz_ca_biplot(ca, col.row="cos2", labelsize = 2) +
       theme_minimal()

ggsave("image/ca.jpg", width = 27, height = 18, units = "cm")

```

# Analyse mutivariée descriptive

## ACP

## Clustering

grouper les Etats en famille similaire par leur profil d'entreprise


# Eléments d'économétrie



## régression multiple

le modèle formellement s'écrit[^1] de la manière suivante : 


$$
y_i=\beta_0+\sum_{j = 1}^{k} \beta_jx_{i,j} + \epsilon_i
$$
où $y_i$ représente la valeur de l'individu $i$ pour la variable que l'on cherche à expliquer $y$. Les $x_{ij}$ représentent les valeurs de la variable $$j$$ pour l'individu $$i$$ . Les paramètres $$\beta_j$$ sont les valeurs que l'on cherche à estimer, en minimisant la comme du carrée des erreurs. ( principe des moindres carrés)


[^1]:Pour écrire des équations en R markdown [voir](<https://rpruim.github.io/s341/S19/from-class/MathinRmd.html>)



## Premier exemple simple

On s'intéresse à la capitalisation sur le marché en 2024. Il y a de bonnes raisons qu'elle soit liée à celle de l'année précédentes. Examinons la relation. 

```{r}
df_2024<-df_2024%>%
  filter(!is.na(MarketCap_Updated_M.23), !is.na(MarketCap_Updated_M.24))

r<-round(cor(df_2024$MarketCap_Updated_M.23,df_2024$MarketCap_Updated_M.24),3)

ggplot(df_2024, aes(x=MarketCap_Updated_M.23,MarketCap_Updated_M.24))+
  geom_point(alpha=.5, size=.8)+
  geom_smooth(method="lm", size=.5)+
  labs(title = paste0("corrélation : ", r))

```
Si la corrélation est forte, il y a cependant un phénomène d' hétéroscédasticité : la variance n'est pas constante. On peut corriger ceci en transformant nos variables par une fonction logarithme. Le résultat est bien plus régulier !

```{r}
df_2024<-df_2024%>%
  filter(!is.na(MarketCap_Updated_M.23), !is.na(MarketCap_Updated_M.24)) |>
  mutate(MarketCap_Updated_M.23_ln=log(MarketCap_Updated_M.23),
         MarketCap_Updated_M.24_ln=log(MarketCap_Updated_M.24))

r<-round(cor(df_2024$MarketCap_Updated_M.23_ln,df_2024$MarketCap_Updated_M.24_ln),3)

ggplot(df_2024, aes(x=MarketCap_Updated_M.23_ln,MarketCap_Updated_M.24_ln))+
  geom_point(alpha=.5, size=.8)+
  geom_smooth(method="lm", size=.5)+
  labs(title = paste0("corrélation : ", r))

```
Effectuons la régression. 


```{r}
fit<-lm( MarketCap_Updated_M.24_ln ~ MarketCap_Updated_M.23_ln, df_2024)
summary(fit)
```
Il est temps d'introduire d'autre variables. On va se contenter d'introduire deux variables :

- l'accroissement du chiffre d'affaires qui signalent la dynamique de l'entreprise et sa capacité à produire du profit, pour autant que les coût ne croissent en égale proportion.

- le secteur d'activité, une variable qualitative, qui sera recodée par hot encoding au regard d'une catégorie de référence.



```{r}


fit1<-lm( MarketCap_Updated_M.24_ln ~ MarketCap_Updated_M.23_ln+  RevenuePercentChange.24 , df_2024)
fit2<-lm( MarketCap_Updated_M.24_ln ~ MarketCap_Updated_M.23_ln+  RevenuePercentChange.24 + Sector.24, df_2024)

library(jtools)

export_summs(fit, fit1,fit2,
             error_pos="right",
              number_format = "%.3f")
```

Voici la representaion graphique sous forme de tree plot équivalente. 

```{r}

plot_summs(fit,fit1, fit2)


```


On peut souhaiter miux se représenter les effets de chacune des variables . Les diagramme d'effets marginaux sont utiles à cette fin. Ils représente l'effet moyen des modalités d'une variable, sous contrôle des autres variables.


```{r}


effect_plot(fit2, pred = RevenuePercentChange.24, interval = TRUE, plot.points = FALSE)+
  scale_x_log10()

effect_plot(fit2, pred = Sector.24, interval = TRUE, plot.points = FALSE)+coord_flip()


```


# D'autres modèles


D'autres spécifications de modèles de régression ont été proposés en fonction de la nature de la variables qu'on veut étudier

-   variable binaire : modèle logit
-   variable de dénombrement : le modèle de Poisson est fondamental
-   variable de durée : modèle de Cox est devenu un standard

{{< pagebreak >}}

## modèles à décomposition d'erreurs

C'est le cas des panels par exemple: chaque individu observé sur la variable $j$ l'est dans le pays $g$ et au moment $t$ . On parle aussi de modèles hiérarchiques. 

$$
y_{i,g}=\beta_0+\sum_{j = 1}^{k} \beta_jx_{i,j,g} + \rho_t+\mu_g+\epsilon_i
$$


# Références
